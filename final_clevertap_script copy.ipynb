{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS\n",
    "\n",
    "# 1) Connect to Database\n",
    "# 2) Load the JSON\n",
    "# 2) Run the DS query to get the list of users who were onboarded during the given period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2 as ps\n",
    "import json\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import math\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO DB\n",
    "\n",
    "conn = ps.connect(\n",
    "    host='ldc-ios-prod.cluster-ro-c60xechvrdpu.ap-south-1.rds.amazonaws.com',\n",
    "    port=5432,\n",
    "    dbname=\"investorsdb\",\n",
    "    user=\"read_usrinvestors\",\n",
    "    password=\"FS4o#cjpe9H$yC73\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL JSONS\n",
    "\n",
    "MAIN_JSON = {\n",
    "    \"identity\": None, \n",
    "    \"type\": \"event\", \n",
    "    \"ts\": None, \n",
    "    \"evtName\": None, \n",
    "    \"evtData\": None\n",
    "}\n",
    "\n",
    "INV_SIGNUP_S2S = {\n",
    "    'Acquisition channel': 'None',\n",
    "    'Identity': 'None',\n",
    "    'Phone': 'None', \n",
    "    'Sourcing Channel': 'None',\n",
    "    'Platform': 'Retail',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None'\n",
    "}\n",
    "\n",
    "INV_LIVE_KYC_S2S = {\n",
    "    'Identity': 'None',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None'\n",
    "}\n",
    "\n",
    "INV_LEGAL_AUTH_S2S = {\n",
    "    'Identity': 'None',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None'\n",
    "}\n",
    "\n",
    "INV_LISTED_S2S = {\n",
    "    'Identity': 'None',\n",
    "    'Account Status': 'None'\n",
    "}\n",
    "\n",
    "\n",
    "INV_ADD_BANKACCOUNT_S2S = {\n",
    "    'Identity': 'None',\n",
    "    'Bank Name': 'None',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None'\n",
    "}\n",
    "\n",
    "\n",
    "INV_FMPP_S2S = {\n",
    "    'Identity': 'None',\n",
    "    'Scheme ID': 'None',\n",
    "    'FMPP Amount': 'None',\n",
    "    'Maturity Amount': 'None',\n",
    "    'Period': 'None',\n",
    "    'ROI': 'None',\n",
    "    'Maturity Date': 'None',\n",
    "    'FMPP Type': 'None',\n",
    "    'Auto Renew': 'None',\n",
    "    'Payment Method': 'None',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None',\n",
    "    'Account Balance': 'None',\n",
    "    'Maturity Timestamp': 'None'\n",
    "}\n",
    "\n",
    "\n",
    "INV_FMPP_S2S_BULK = {\n",
    "    'Identity': 'None',\n",
    "    'Scheme ID': 'None',\n",
    "    'FMPP Amount': 'None',\n",
    "    'Maturity Amount': 'None',\n",
    "    'Period': 'None',\n",
    "    'ROI': 'None',\n",
    "    'Maturity Date': 'None',\n",
    "    'FMPP Type': 'None',\n",
    "    'Auto Renew': 'None',\n",
    "    'Payment Method': 'None',\n",
    "    'Status': '1',\n",
    "    'Reason': 'None',\n",
    "    'Account Balance': 'None',\n",
    "    'Maturity Timestamp': 'None',\n",
    "    'Order ID': 'None'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE THE MONTH\n",
    "\n",
    "months = {\n",
    "    'OCT_2024': ['2024-10-24 18:30:00', '2024-10-27 18:29:59']\n",
    "}\n",
    "dates = months.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-10-24 18:30:00'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = list(months.values())[0]\n",
    "dates\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE USER FROM THIS QUERY\n",
    "\n",
    "def signup_ds(month, key, month_key):\n",
    "    query = f\"\"\"select lc.user_id, date(la.created_date)  from lendenapp_customuser lc\n",
    "                inner join lendenapp_user_source_group lusg on lc.id = lusg.user_id \n",
    "                inner join lendenapp_account la on la.user_source_group_id = lusg.id \n",
    "                where lusg.source_id = 7 and la.created_date between '{dates[0]}' and '{dates[1]}';\n",
    "            \"\"\"\n",
    "    print(query)\n",
    "    \n",
    "    signup_query = pd.read_sql_query(query, con=conn)\n",
    "    signup_query.to_csv(f'All New Data for CT/Users from CSV/SU/{month_key}_{key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORING SIGNUP for OCT_2024\n",
      "select lc.user_id, date(la.created_date)  from lendenapp_customuser lc\n",
      "                inner join lendenapp_user_source_group lusg on lc.id = lusg.user_id \n",
      "                inner join lendenapp_account la on la.user_source_group_id = lusg.id \n",
      "                where lusg.source_id = 7 and la.created_date between '2024-10-24 18:30:00' and '2024-10-27 18:29:59';\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/2809698353.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  signup_query = pd.read_sql_query(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "#RUN DS QUERIES\n",
    "\n",
    "for key, value in months.items():\n",
    "    print(f\"STORING SIGNUP for {key}\")\n",
    "    signup_ds(value, 'SU', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET ALL USERS FOR THAT MONTH\n",
    "\n",
    "def list_files_func(folder_path):\n",
    "    return glob.glob(os.path.join(folder_path, '*'))\n",
    "\n",
    "def create_data_csvs(folder):\n",
    "    list_files = list_files_func(f'All New Data for CT/Users from CSV/{folder}')\n",
    "\n",
    "    all_users = []\n",
    "\n",
    "    for index in range(len(list_files)):\n",
    "        print(list_files[index])\n",
    "        data = pd.read_csv(list_files[index])\n",
    "        all_users.extend(list(data['user_id']))\n",
    "    \n",
    "    return all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All New Data for CT/Users from CSV/SU/OCT_2024_SU.csv\n"
     ]
    }
   ],
   "source": [
    "users = create_data_csvs('SU')\n",
    "users = tuple(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL DATA\n",
    "\n",
    "def get_data_for_signup(data):\n",
    "    query = f\"\"\"\n",
    "                SELECT \n",
    "                lc.user_id as \"Identity\", \n",
    "                (la.created_date) as ts,\n",
    "                lc.mobile_number as \"Phone\", \n",
    "                ls.source_name as \"Sourcing Channel\" \n",
    "                FROM lendenapp_customuser lc\n",
    "                INNER JOIN lendenapp_user_source_group lusg ON lc.id = lusg.user_id\n",
    "                INNER JOIN lendenapp_account la ON la.user_source_group_id = lusg.id\n",
    "                INNER JOIN lendenapp_source ls ON ls.id = lusg.source_id\n",
    "                WHERE lc.user_id IN {data} AND lusg.source_id = 7\n",
    "            \"\"\"\n",
    "    \n",
    "    signups = pd.read_sql_query(query, con=conn)\n",
    "    return signups\n",
    "\n",
    "\n",
    "def get_data_for_kyc(data, dates):\n",
    "    query = f\"\"\"\n",
    "                SELECT lt.checklist, lt.history_date, lc.user_id, lt.created_by_id \n",
    "                FROM lendenapp_historicaltask lt\n",
    "                LEFT JOIN lendenapp_user_source_group lusg ON lt.user_source_group_id = lusg.id\n",
    "                LEFT JOIN lendenapp_customuser lc ON lc.id = lusg.user_id\n",
    "                WHERE lusg.source_id = 7 AND lt.history_date between '{dates[0]}' and '{dates[1]}';\n",
    "            \"\"\"\n",
    "    kycs = pd.read_sql_query(query, con=conn)\n",
    "    kycs.to_csv('KYC History Data.csv')\n",
    "\n",
    "    kyc = {'Identity': [], 'ts': []}\n",
    "    data_for_kyc = pd.read_csv('KYC History Data.csv')\n",
    "\n",
    "    checklist = list(data_for_kyc['checklist'])\n",
    "    user_id = list(data_for_kyc['user_id'])\n",
    "    ts = list(data_for_kyc['history_date'])\n",
    "\n",
    "    final = []\n",
    "    for index in range(len(checklist)):\n",
    "        if type(checklist[index]) != float:  # Skip NaN values\n",
    "            current_checklist = checklist[index]\n",
    "            try:\n",
    "                # Convert string to valid JSON object\n",
    "                current_checklist = current_checklist.replace(\"'\", '\"')  # Convert single quotes to double quotes if needed\n",
    "                current_checklist = json.loads(current_checklist)  # Convert to a dictionary\n",
    "\n",
    "                # Check if 'completed_steps' exists and if the last step is 'LIVE KYC'\n",
    "                if current_checklist and 'completed_steps' in current_checklist and len(current_checklist['completed_steps'])>0 and current_checklist['completed_steps'][-1] == 'LIVE KYC':\n",
    "                    kyc['Identity'].append(user_id[index])\n",
    "                    kyc['ts'].append(ts[index])\n",
    "                    final.append(user_id[index])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding JSON for user_id {user_id[index]}: {e}\")\n",
    "                continue  # Skip rows with invalid JSON\n",
    "\n",
    "    kyc_df = pd.DataFrame(kyc, columns=['Identity', 'ts'])\n",
    "    \n",
    "    return kyc_df\n",
    "\n",
    "\n",
    "def get_data_for_legal(data, dates):\n",
    "    query = f\"\"\"\n",
    "                SELECT lt.checklist, lt.history_date, lc.user_id, lt.created_by_id from lendenapp_historicaltask lt\n",
    "                LEFT JOIN lendenapp_user_source_group lusg ON lt.user_source_group_id = lusg.id\n",
    "                LEFT JOIN lendenapp_customuser lc ON lc.id = lusg.user_id\n",
    "                where lusg.source_id = 7 and lt.history_date between '{dates[0]}' and '{dates[1]}';\n",
    "            \"\"\"\n",
    "    legs = pd.read_sql_query(query, con=conn)\n",
    "    legs.to_csv('Legal History Data.csv')\n",
    "\n",
    "    legal = {'Identity': [], 'ts':[]}\n",
    "    data_for_legal = pd.read_csv('Legal History Data.csv')\n",
    "\n",
    "    checklist = list(data_for_legal['checklist'])\n",
    "    user_id = list(data_for_legal['user_id'])\n",
    "    ts = list(data_for_legal['history_date'])\n",
    "\n",
    "\n",
    "    final = []\n",
    "    for index in range(len(checklist)):\n",
    "        if type(checklist[index]) != float:  # Skip NaN values\n",
    "            current_checklist = checklist[index]\n",
    "            try:\n",
    "                # Convert string to valid JSON object\n",
    "                current_checklist = current_checklist.replace(\"'\", '\"')  # Convert single quotes to double quotes if needed\n",
    "                current_checklist = json.loads(current_checklist)  # Convert to a dictionary\n",
    "\n",
    "                # Check if 'completed_steps' exists and if the last step is 'LIVE KYC'\n",
    "                if current_checklist and 'completed_steps' in current_checklist and len(current_checklist['completed_steps'])>0 and current_checklist['completed_steps'][-1] == 'LEGAL AUTHORIZATION':\n",
    "                    legal['Identity'].append(user_id[index])\n",
    "                    legal['ts'].append(ts[index])\n",
    "                    final.append(user_id[index])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding JSON for user_id {user_id[index]}: {e}\")\n",
    "                continue  # Skip rows with invalid JSON\n",
    "\n",
    "    legal = pd.DataFrame(legal, columns=['Identity', 'ts'])\n",
    "    return legal\n",
    "\n",
    "def get_data_for_bank(data, dates):\n",
    "    query = f\"\"\"\n",
    "                SELECT lba.created_date as \"ts\", \n",
    "                lb.name as \"Bank Name\",\n",
    "                lc.user_id as \"Identity\"\n",
    "                FROM lendenapp_bankaccount lba\n",
    "                LEFT JOIN lendenapp_bank lb ON lb.id = lba.bank_id\n",
    "                LEFT JOIN lendenapp_customuser lc ON lc.id = lba.user_id\n",
    "                LEFT JOIN lendenapp_user_source_group lusg ON lba.user_source_group_id = lusg.id\n",
    "                where lusg.source_id = 7 and lba.created_date between '{dates[0]}' and '{dates[1]}'\n",
    "            \"\"\"\n",
    "    banks = pd.read_sql_query(query, con=conn)\n",
    "    return banks\n",
    "\n",
    "\n",
    "def get_data_for_listed(data):\n",
    "    query = f\"\"\"\n",
    "                SELECT lba.created_date AS \"ts\", \n",
    "                lc.user_id AS \"Identity\"\n",
    "                FROM lendenapp_bankaccount lba\n",
    "                INNER JOIN lendenapp_customuser lc ON lc.id = lba.user_id\n",
    "                INNER JOIN lendenapp_user_source_group lusg ON lba.user_source_group_id = lusg.id\n",
    "                INNER JOIN lendenapp_account la ON lusg.id = la.user_source_group_id\n",
    "                WHERE lusg.source_id = 7 AND la.listed_date between '2024-10-25' and '2024-10-27';\n",
    "            \"\"\"\n",
    "    listeds = pd.read_sql_query(query, con=conn)\n",
    "\n",
    "    listeds['Account Status'] = 'LISTED'\n",
    "    return listeds\n",
    "\n",
    "#\n",
    "\n",
    "def get_manual_lending_data(data, dates):\n",
    "    print(dates)\n",
    "    query = f\"\"\"\n",
    "                SELECT lc.user_id as \"Identity\",\n",
    "                tis.urn_id as \"Scheme ID\", tis.investment_amount as \"FMPP Amount\",\n",
    "                tis.expected_temp_interest_repayment_sum as \"Maturity Amount\", tsm.tenure as \"Period\",\n",
    "                tis.expected_roi as \"ROI\", tis.maturity_date as \"Maturity Date\",\n",
    "                tis.auto_renew as \"Auto Renew\", \n",
    "                EXTRACT(EPOCH FROM tis.maturity_date) as \"Maturity Timestamp\", \n",
    "                tis.order_id as \"Order ID\",\n",
    "                tis.created_dtm as \"ts\"\n",
    "                FROM fmpp.t_investor_scheme tis \n",
    "                INNER JOIN fmpp.t_investors ts ON tis.investor_id = ts.id \n",
    "                INNER JOIN lendenapp_customuser lc ON lc.user_id = ts.source_id\n",
    "                INNER JOIN fmpp.t_scheme_master tsm ON tsm.id = tis.scheme_master_id\n",
    "                INNER JOIN lendenapp_user_source_group lusg ON lusg.user_id = lc.id\n",
    "                WHERE tis.order_id IS NOT null\n",
    "                AND tis.partner_code_id = 49 AND \n",
    "                tis.created_dtm between '2024-10-24 18:30:00' and '2024-10-27 18:29:59'\n",
    "                AND tis.deleted is null AND lusg.source_id = 7;\n",
    "            \"\"\"\n",
    "    \n",
    "    ml_schemes = pd.read_sql_query(query, con=conn)\n",
    "    \n",
    "    ml_schemes_grouped = ml_schemes.groupby('Order ID').agg({\n",
    "            'Identity': 'first',\n",
    "            'Auto Renew': 'first',\n",
    "            'ts': 'first',\n",
    "            'Order ID': 'first',\n",
    "            'FMPP Amount': 'sum',\n",
    "            'ROI': 'mean',\n",
    "            'Maturity Date': 'max',\n",
    "            'Scheme ID': 'first',\n",
    "            'Period': 'first',\n",
    "            'Maturity Amount': 'first',\n",
    "            'Maturity Timestamp': 'max'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "    ml_schemes_grouped['FMPP Type'] = 'BULK_LENDING'\n",
    "\n",
    "    ml_schemes_grouped.to_csv('xyz.com')\n",
    "        \n",
    "    return ml_schemes_grouped\n",
    "\n",
    "def get_scheme_data(data, dates):\n",
    "    print(dates)\n",
    "    query = f\"\"\"\n",
    "                SELECT lc.user_id as \"Identity\",\n",
    "                tis.urn_id as \"Scheme ID\", tis.investment_amount as \"FMPP Amount\",\n",
    "                tis.expected_temp_interest_repayment_sum as \"Maturity Amount\", tsm.tenure as \"Period\",\n",
    "                tis.expected_roi as \"ROI\", tis.maturity_date as \"Maturity Date\",\n",
    "                tis.auto_renew as \"Auto Renew\", \n",
    "                EXTRACT(EPOCH FROM tis.maturity_date) as \"Maturity Timestamp\", \n",
    "                tis.created_dtm as \"ts\",\n",
    "                tmp.key_2 as \"FMPP Type\"\n",
    "                FROM fmpp.t_investor_scheme tis\n",
    "                INNER JOIN fmpp.t_investors ts ON tis.investor_id = ts.id \n",
    "                INNER JOIN lendenapp_customuser lc ON lc.user_id = ts.source_id\n",
    "                INNER JOIN fmpp.t_mst_parameter tmp ON tmp.id = tis.investment_type_id\n",
    "                INNER JOIN fmpp.t_scheme_master tsm ON tsm.id = tis.scheme_master_id\n",
    "                INNER JOIN lendenapp_user_source_group lusg ON lusg.user_id = lc.id\n",
    "                WHERE tis.order_id IS null\n",
    "                AND tis.partner_code_id = 49 \n",
    "                AND tis.created_dtm between '2024-10-24 18:30:00' and '2024-10-27 18:29:59'\n",
    "                AND tis.deleted is null AND lusg.source_id = 7;\n",
    "            \"\"\"\n",
    "    \n",
    "    schemes = pd.read_sql_query(query, con=conn)\n",
    "\n",
    "    schemes.to_csv('abc.com')\n",
    "\n",
    "    return schemes\n",
    "\n",
    "\n",
    "#'2022-07-31 18:30:00' and '2022-12-31 18:29:59'\n",
    "#'2022-12-31 18:30:00' and '2023-06-30 18:29:59'\n",
    "#'2023-06-30 18:30:00' and '2023-12-31 18:29:59'\n",
    "#'2023-12-31 18:30:00' and '2024-06-30 18:29:59'\n",
    "#'2024-06-30 18:30:00' and '2024-10-14 18:29:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_csvs(folder, function, data, dates, file_name, dates_needed):\n",
    "    if dates_needed:\n",
    "        given_data=function(data, dates)\n",
    "    else:\n",
    "        given_data=function(data)\n",
    "    file_name_join = file_name+'DATA'\n",
    "    given_data.to_csv(f'All New Data for CT/Users from CSV/{folder}/{file_name_join}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  signups = pd.read_sql_query(query, con=conn)\n",
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  kycs = pd.read_sql_query(query, con=conn)\n",
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:70: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  legs = pd.read_sql_query(query, con=conn)\n",
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:114: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  banks = pd.read_sql_query(query, con=conn)\n",
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:128: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  listeds = pd.read_sql_query(query, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-10-24 18:30:00', '2024-10-27 18:29:59']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:157: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ml_schemes = pd.read_sql_query(query, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-10-24 18:30:00', '2024-10-27 18:29:59']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/lgfhb72d6_q0t0_xmyt0d8cc0000gp/T/ipykernel_4775/159060056.py:202: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schemes = pd.read_sql_query(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "create_data_csvs('SU', get_data_for_signup, users, dates, 'SU', dates_needed=False)\n",
    "create_data_csvs('KYC', get_data_for_kyc, users,dates, 'KYC', dates_needed=True)\n",
    "create_data_csvs('LA', get_data_for_legal, users,dates, 'LA', dates_needed=True)\n",
    "create_data_csvs('BA', get_data_for_bank, users,dates, 'BA', dates_needed=True)\n",
    "create_data_csvs('LI', get_data_for_listed, users,dates, 'LI', dates_needed=False)\n",
    "create_data_csvs('FMPP/BL', get_manual_lending_data, users, dates, 'BL', dates_needed=True)\n",
    "create_data_csvs('FMPP', get_scheme_data, users, dates, 'FMPP', dates_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(given_time):\n",
    "    epoch_timestamp = int(given_time.timestamp())\n",
    "    return epoch_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(dataframe, event_json, main_json, event_name, columns_to_update):\n",
    "    final_data = []\n",
    "    all_users = []\n",
    "    p_count=0\n",
    "    n_count=0\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        try:\n",
    "            if dataframe['Identity'][index]:\n",
    "                current_json = copy.deepcopy(main_json)\n",
    "                new_json = copy.deepcopy(event_json)\n",
    "                new_json.update(\n",
    "                    {\n",
    "                        column: dataframe[column][index]\n",
    "                        for column in columns_to_update\n",
    "                    }\n",
    "                )\n",
    "                current_json.update(\n",
    "                    {\n",
    "                        'identity': dataframe['Identity'][index],\n",
    "                        'ts': convert_to_timestamp(datetime.fromisoformat(dataframe['ts'][index])),\n",
    "                        'evtName': event_name,\n",
    "                        'evtData': new_json,\n",
    "                    }\n",
    "                )\n",
    "                if 'Auto Renew' in new_json:\n",
    "                    new_json.update({\n",
    "                        'Auto Renew': str(new_json['Auto Renew'])\n",
    "                    })\n",
    "                if current_json['identity']:\n",
    "                    final_data.append(current_json)\n",
    "                    if current_json['identity'] not in all_users:\n",
    "                        all_users.append(current_json['identity'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e, index, dataframe['Identity'][index])\n",
    "\n",
    "    return final_data, len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_pack(event, event_json, main_json, event_name, columns):\n",
    "    list_files = list_files_func(f'All New Data for CT/Users from CSV/{event}')\n",
    "    data_json = {}\n",
    "    user_json = {}\n",
    "    variable_key_name = f'{event}_month'\n",
    "    try:\n",
    "        for index in range(len(list_files)):\n",
    "            if list_files[index][-8:] == 'DATA.csv':\n",
    "                current_csv = pd.read_csv(list_files[index])\n",
    "                current_month, all_users = create_data(current_csv, event_json, main_json, event_name, columns)\n",
    "                variable_key = f'{variable_key_name}_{str(index+1)}'\n",
    "                data_json[variable_key] = current_month\n",
    "                user_json[variable_key] = all_users\n",
    "    except Exception as e:\n",
    "        print(e, list_files[index])\n",
    "    \n",
    "    return data_json, user_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "signup_json = create_final_pack('SU', INV_SIGNUP_S2S, MAIN_JSON, 'INV SignUp S2S', ['Identity', 'Phone', 'Sourcing Channel'])\n",
    "kyc_json = create_final_pack('KYC', INV_LIVE_KYC_S2S, MAIN_JSON, 'INV Live KYC S2S', ['Identity'])\n",
    "la_json = create_final_pack('LA', INV_LEGAL_AUTH_S2S, MAIN_JSON, 'INV Legal Authorization S2S', ['Identity'])\n",
    "bank_json = create_final_pack('BA', INV_ADD_BANKACCOUNT_S2S, MAIN_JSON, 'INV Bank Account S2S', ['Identity', 'Bank Name'])\n",
    "listed_json = create_final_pack('LI', INV_LISTED_S2S, MAIN_JSON, 'INV Listed S2S', ['Identity', 'Account Status'])\n",
    "bl_json = create_final_pack('FMPP/BL', INV_FMPP_S2S_BULK, MAIN_JSON, 'INV CREATE FMPP SCHEME S2S', ['Identity', 'Scheme ID', 'FMPP Amount','Maturity Amount','Period','ROI', 'Maturity Date', 'FMPP Type', 'Auto Renew', 'Maturity Timestamp', 'Order ID'])\n",
    "al_json = create_final_pack('FMPP', INV_FMPP_S2S, MAIN_JSON, 'INV CREATE FMPP SCHEME S2S', ['Identity', 'Scheme ID', 'FMPP Amount','Maturity Amount','Period','ROI', 'Maturity Date', 'FMPP Type', 'Auto Renew', 'Maturity Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_split(json_data, max_record):\n",
    "    highest = len(json_data)\n",
    "\n",
    "    batches = math.ceil(highest / max_record)\n",
    "    if batches>0:\n",
    "        size_per_batch = highest // batches\n",
    "\n",
    "        return size_per_batch\n",
    "    \n",
    "def create_batches(ct_json, batch_size):\n",
    "    final = []\n",
    "    for index in range(0, len(ct_json), batch_size):\n",
    "        x={}\n",
    "        x[\"d\"]=(ct_json[index:index+batch_size])\n",
    "        final.append(x)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleverTap:\n",
    "    def __init__(self, account_id, passcode):\n",
    "        self.account_id = account_id\n",
    "        self.passcode = passcode\n",
    "\n",
    "    @property\n",
    "    def headers(self):\n",
    "        return {\n",
    "            'X-CleverTap-Account-Id': self.account_id,\n",
    "            'X-CleverTap-Passcode': self.passcode,\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "        }\n",
    "    def upload_events(self, url, data):\n",
    "        data = str(data)\n",
    "        headers = self.headers\n",
    "        response = requests.post(url=url, headers=headers, data=data)\n",
    "        return response\n",
    "\n",
    "    def upload_user(self, url, data):\n",
    "        data = str(data)\n",
    "        headers = self.headers\n",
    "        response = requests.post(url=url, headers=headers, data=data)\n",
    "        return response\n",
    "\n",
    "c=CleverTap('R95-W74-766Z', 'ERO-QMA-IEUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOB = 0\n",
    "def send_batch(batch):\n",
    "    global c\n",
    "    global GLOB\n",
    "    url = \"CleverTap API endpoint\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
    "\n",
    "    response=c.upload_events(\"https://in1.api.clevertap.com/1/upload\", batch)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Batch successfully uploaded\")\n",
    "    else:\n",
    "        print(f\"Failed to upload batch: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_push(final):      \n",
    "    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "        executor.map(send_batch, final)\n",
    "\n",
    "\n",
    "def push_data(data):\n",
    "    # for data in data_array:\n",
    "    batch_size = batch_split(data, 1000)\n",
    "    if data and batch_size and batch_size>0:\n",
    "        final = create_batches(ct_json=data, batch_size=batch_size)\n",
    "        final_push(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploadedBatch successfully uploaded\n",
      "\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n",
      "Batch successfully uploaded\n"
     ]
    }
   ],
   "source": [
    "final_jsons = [signup_json[0], kyc_json[0], la_json[0], bank_json[0], al_json[0], bl_json[0]]\n",
    "\n",
    "for json in final_jsons:\n",
    "    for key, value in json.items():\n",
    "        push_data(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(al_json):\n",
    "    users = set()\n",
    "    amount = 0\n",
    "    for i in range(len(al_json[0]['FMPP_month_2'])):\n",
    "        current=al_json[0]['FMPP_month_2'][i]['evtData']\n",
    "        users.add(current['Identity'])\n",
    "        amount+=current['FMPP Amount']\n",
    "\n",
    "    return len(users), amount, len(al_json[0]['FMPP_month_2'])\n",
    "\n",
    "\n",
    "def get_stats_bl(bl_json):\n",
    "    users = set()\n",
    "    amount = 0\n",
    "    for i in range(len(bl_json[0]['FMPP/BL_month_1'])):\n",
    "        current=bl_json[0]['FMPP/BL_month_1'][i]['evtData']\n",
    "        users.add(current['Identity'])\n",
    "        amount+=current['FMPP Amount']\n",
    "\n",
    "    return len(users), amount, len(bl_json[0]['FMPP/BL_month_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGN UPS:  \t 4800\n",
      "LIVE KYC:  \t 2593\n",
      "LEGAL AUTH:  \t 2491\n",
      "BANK ACCOUNT:  \t 2532\n",
      "USER LISTED:  \t 2537\n",
      "\n",
      "\n",
      "CR SCHEMES \t SCHEMES AMOUNT USERS\n",
      "BULK LENDING:  \t (249, 1781718.0, 818)\n",
      "OTHER SCHEMES:   (16, 1667492.0, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"SIGN UPS: \", \"\\t\", len(signup_json[0]['SU_month_1']))\n",
    "print(\"LIVE KYC: \", \"\\t\", len(kyc_json[0]['KYC_month_1']))\n",
    "print(\"LEGAL AUTH: \", \"\\t\", len(la_json[0]['LA_month_1']))\n",
    "print(\"BANK ACCOUNT: \", \"\\t\", len(bank_json[0]['BA_month_1']))\n",
    "print(\"USER LISTED: \", \"\\t\", len(listed_json[0]['LI_month_1']))\n",
    "print(\"\\n\")\n",
    "print(\"CR SCHEMES\", \"\\t\", \"SCHEMES AMOUNT USERS\")\n",
    "print(\"BULK LENDING: \",\"\\t\", get_stats_bl(bl_json))\n",
    "print(\"OTHER SCHEMES:  \", get_stats(al_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Decimal is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mDecimal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.54\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Decimal is not JSON serializable"
     ]
    }
   ],
   "source": [
    "x=json.dumps({'x':Decimal(1.54)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal, decimal_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_default(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    raise TypeError(\"Object of type Decimal is not JSON serializable\")\n",
    "\n",
    "# Convert event_data to JSON, handling Decimal values\n",
    "params = {\n",
    "    'status': 'SUCCESS',\n",
    "    'event_data': json.dumps(event_data, default=decimal_default)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = {'x': Decimal('4.59'), 'y':'Glen'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'SUCCESS', 'event_data': '{\"x\": 4.59, \"y\": \"Glen\"}'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
